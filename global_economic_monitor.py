"""
global_economic_monitor.py - å…¨çƒç¶“æ¿ŸæŒ‡æ¨™ç›£æ§ç³»çµ±
ç²å–å’Œåˆ†æå…¨çƒä¸»è¦è‚¡å¸‚æŒ‡æ•¸ã€ç¶“æ¿ŸæŒ‡æ¨™
"""
import os
import json
import time
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import logging

logger = logging.getLogger(__name__)

class GlobalEconomicMonitor:
    """å…¨çƒç¶“æ¿ŸæŒ‡æ¨™ç›£æ§å™¨"""
    
    def __init__(self, cache_dir: str = 'cache/global'):
        """åˆå§‹åŒ–å…¨çƒç¶“æ¿Ÿç›£æ§å™¨"""
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        # å…¨çƒæŒ‡æ•¸ä»£ç¢¼æ˜ å°„
        self.global_indices = {
            # ç¾åœ‹å¸‚å ´
            'SPX': {'name': 'S&P 500', 'region': 'US', 'symbol': '^GSPC'},
            'NASDAQ': {'name': 'NASDAQ', 'region': 'US', 'symbol': '^IXIC'},
            'DOW': {'name': 'Dow Jones', 'region': 'US', 'symbol': '^DJI'},
            'VIX': {'name': 'VIXææ…ŒæŒ‡æ•¸', 'region': 'US', 'symbol': '^VIX'},
            
            # äºæ´²å¸‚å ´
            'NIKKEI': {'name': 'æ—¥ç¶“225', 'region': 'Japan', 'symbol': '^N225'},
            'HSI': {'name': 'æ†ç”ŸæŒ‡æ•¸', 'region': 'HongKong', 'symbol': '^HSI'},
            'KOSPI': {'name': 'éŸ“åœ‹KOSPI', 'region': 'Korea', 'symbol': '^KS11'},
            'TWII': {'name': 'å°ç£åŠ æ¬Š', 'region': 'Taiwan', 'symbol': '^TWII'},
            
            # æ­æ´²å¸‚å ´
            'DAX': {'name': 'å¾·åœ‹DAX', 'region': 'Germany', 'symbol': '^GDAXI'},
            'FTSE': {'name': 'è‹±åœ‹FTSE', 'region': 'UK', 'symbol': '^FTSE'},
            'CAC': {'name': 'æ³•åœ‹CAC', 'region': 'France', 'symbol': '^FCHI'},
            
            # å¤§å®—å•†å“å’Œè²¨å¹£
            'GOLD': {'name': 'é»ƒé‡‘', 'region': 'Commodity', 'symbol': 'GC=F'},
            'OIL': {'name': 'åŸæ²¹', 'region': 'Commodity', 'symbol': 'CL=F'},
            'DXY': {'name': 'ç¾å…ƒæŒ‡æ•¸', 'region': 'Currency', 'symbol': 'DX-Y.NYB'},
        }
        
        # ç¶“æ¿ŸæŒ‡æ¨™
        self.economic_indicators = {
            'US10Y': {'name': 'ç¾åœ‹10å¹´æœŸå…¬å‚µ', 'type': 'bond'},
            'USD_TWD': {'name': 'ç¾å…ƒå…Œå°å¹£', 'type': 'fx'},
            'USD_JPY': {'name': 'ç¾å…ƒå…Œæ—¥åœ“', 'type': 'fx'},
        }
        
        # API è¨­å®šï¼ˆå¯ä½¿ç”¨ Yahoo Finance æˆ–å…¶ä»–å…è²»APIï¼‰
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # å¿«å–æ™‚é–“ï¼ˆåˆ†é˜ï¼‰
        self.cache_duration = {
            'market_hours': 5,      # äº¤æ˜“æ™‚é–“5åˆ†é˜
            'after_hours': 30,      # ç›¤å¾Œ30åˆ†é˜
            'weekend': 240          # é€±æœ«4å°æ™‚
        }
    
    def get_global_indices_data(self) -> Dict[str, Any]:
        """ç²å–å…¨çƒä¸»è¦æŒ‡æ•¸æ•¸æ“š"""
        cache_key = 'global_indices_data'
        
        # æª¢æŸ¥å¿«å–
        cached_data = self._load_cache(cache_key)
        if cached_data:
            return cached_data
        
        logger.info("æ­£åœ¨ç²å–å…¨çƒæŒ‡æ•¸æ•¸æ“š...")
        
        indices_data = {}
        
        # ä½¿ç”¨Yahoo Finance APIç²å–æ•¸æ“š
        for code, info in self.global_indices.items():
            try:
                data = self._fetch_yahoo_data(info['symbol'])
                if data:
                    indices_data[code] = {
                        'name': info['name'],
                        'region': info['region'],
                        'price': data['price'],
                        'change': data['change'],
                        'change_percent': data['change_percent'],
                        'timestamp': data['timestamp']
                    }
                time.sleep(0.5)  # é¿å…APIé™åˆ¶
            except Exception as e:
                logger.error(f"ç²å– {code} æ•¸æ“šå¤±æ•—: {e}")
                continue
        
        # ä¿å­˜å¿«å–
        if indices_data:
            self._save_cache(cache_key, indices_data)
        
        return indices_data
    
    def _fetch_yahoo_data(self, symbol: str) -> Optional[Dict[str, Any]]:
        """å¾Yahoo Financeç²å–æ•¸æ“š"""
        try:
            # Yahoo Finance æŸ¥è©¢API
            url = f"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}"
            params = {
                'range': '1d',
                'interval': '1m',
                'includePrePost': 'true'
            }
            
            response = requests.get(url, headers=self.headers, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            if 'chart' not in data or not data['chart']['result']:
                return None
            
            result = data['chart']['result'][0]
            meta = result['meta']
            
            current_price = meta.get('regularMarketPrice', 0)
            previous_close = meta.get('previousClose', 0)
            
            if current_price and previous_close:
                change = current_price - previous_close
                change_percent = (change / previous_close) * 100
                
                return {
                    'price': round(current_price, 2),
                    'change': round(change, 2),
                    'change_percent': round(change_percent, 2),
                    'timestamp': datetime.now().isoformat()
                }
            
        except Exception as e:
            logger.error(f"ç²å–Yahooæ•¸æ“šå¤±æ•— {symbol}: {e}")
        
        return None
    
    def analyze_global_sentiment(self) -> Dict[str, Any]:
        """åˆ†æå…¨çƒå¸‚å ´æƒ…ç·’"""
        indices_data = self.get_global_indices_data()
        
        if not indices_data:
            return {'error': 'ç„¡æ³•ç²å–å…¨çƒæŒ‡æ•¸æ•¸æ“š'}
        
        # å€åŸŸåˆ†æ
        regional_performance = {}
        regions = ['US', 'Japan', 'HongKong', 'Korea', 'Germany', 'UK']
        
        for region in regions:
            region_indices = {k: v for k, v in indices_data.items() if v['region'] == region}
            if region_indices:
                changes = [data['change_percent'] for data in region_indices.values()]
                regional_performance[region] = {
                    'avg_change': round(np.mean(changes), 2),
                    'indices_count': len(region_indices),
                    'positive_count': sum(1 for c in changes if c > 0)
                }
        
        # å…¨çƒæƒ…ç·’è©•åˆ†è¨ˆç®—
        all_changes = []
        for data in indices_data.values():
            if data['region'] != 'Commodity':  # æ’é™¤å¤§å®—å•†å“
                all_changes.append(data['change_percent'])
        
        if all_changes:
            avg_change = np.mean(all_changes)
            positive_ratio = sum(1 for c in all_changes if c > 0) / len(all_changes)
            
            # æƒ…ç·’è©•åˆ† (-5 åˆ° +5)
            sentiment_score = avg_change * 0.5 + (positive_ratio - 0.5) * 4
            
            # VIXå½±éŸ¿
            vix_data = indices_data.get('VIX')
            if vix_data:
                vix_value = vix_data['price']
                if vix_value > 30:
                    sentiment_score -= 2  # é«˜ææ…Œæ¸›åˆ†
                elif vix_value < 15:
                    sentiment_score += 1  # ä½ææ…ŒåŠ åˆ†
        else:
            sentiment_score = 0
        
        # æƒ…ç·’åˆ†é¡
        if sentiment_score > 2:
            sentiment = "éå¸¸æ¨‚è§€"
        elif sentiment_score > 0.5:
            sentiment = "æ¨‚è§€"
        elif sentiment_score > -0.5:
            sentiment = "ä¸­æ€§"
        elif sentiment_score > -2:
            sentiment = "æ‚²è§€"
        else:
            sentiment = "éå¸¸æ‚²è§€"
        
        return {
            'global_sentiment_score': round(sentiment_score, 2),
            'sentiment': sentiment,
            'regional_performance': regional_performance,
            'vix_value': indices_data.get('VIX', {}).get('price', 0),
            'total_indices': len(indices_data),
            'positive_indices': sum(1 for d in indices_data.values() if d.get('change_percent', 0) > 0),
            'analysis_time': datetime.now().isoformat()
        }
    
    def get_correlation_analysis(self) -> Dict[str, Any]:
        """åˆ†æå„æŒ‡æ•¸é–“çš„ç›¸é—œæ€§"""
        try:
            # é€™è£¡å¯ä»¥æ“´å±•ç‚ºå¾æ­·å²æ•¸æ“šè¨ˆç®—çœŸå¯¦ç›¸é—œæ€§
            # æš«æ™‚ä½¿ç”¨ç†è«–ç›¸é—œæ€§
            correlations = {
                'SPX_NASDAQ': {'correlation': 0.85, 'description': 'ç¾åœ‹å¤§å‹è‚¡èˆ‡ç§‘æŠ€è‚¡é«˜åº¦ç›¸é—œ'},
                'SPX_TWII': {'correlation': 0.65, 'description': 'ç¾è‚¡å°å°è‚¡æœ‰ä¸­é«˜åº¦å½±éŸ¿'},
                'NIKKEI_HSI': {'correlation': 0.72, 'description': 'äºæ´²å¸‚å ´é–“ç›¸é—œæ€§è¼ƒé«˜'},
                'VIX_SPX': {'correlation': -0.78, 'description': 'ææ…ŒæŒ‡æ•¸èˆ‡ç¾è‚¡å‘ˆè² ç›¸é—œ'},
                'GOLD_DXY': {'correlation': -0.68, 'description': 'é»ƒé‡‘èˆ‡ç¾å…ƒæŒ‡æ•¸è² ç›¸é—œ'}
            }
            
            return {
                'correlations': correlations,
                'analysis_note': 'åŸºæ–¼æ­·å²æ•¸æ“šçš„ç†è«–ç›¸é—œæ€§ï¼Œå¯¦éš›ç›¸é—œæ€§æœƒéš¨å¸‚å ´ç’°å¢ƒè®ŠåŒ–',
                'update_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ç›¸é—œæ€§åˆ†æå¤±æ•—: {e}")
            return {'error': 'ç›¸é—œæ€§åˆ†æå¤±æ•—'}
    
    def get_economic_calendar(self) -> List[Dict[str, Any]]:
        """ç²å–é‡è¦ç¶“æ¿Ÿäº‹ä»¶æ—¥æ›†ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        # é€™è£¡å¯ä»¥æ¥å…¥çœŸå¯¦çš„ç¶“æ¿Ÿæ—¥æ›†API
        # æš«æ™‚ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
        upcoming_events = [
            {
                'date': '2025-06-01',
                'event': 'ç¾åœ‹éè¾²å°±æ¥­æ•¸æ“š',
                'importance': 'HIGH',
                'impact': 'ç¾è‚¡ã€ç¾å…ƒ'
            },
            {
                'date': '2025-06-03',
                'event': 'æ­æ´²å¤®è¡Œåˆ©ç‡æ±ºè­°',
                'importance': 'HIGH',
                'impact': 'æ­è‚¡ã€æ­å…ƒ'
            },
            {
                'date': '2025-06-05',
                'event': 'æ—¥æœ¬å¤®è¡Œè²¨å¹£æ”¿ç­–',
                'importance': 'MEDIUM',
                'impact': 'æ—¥è‚¡ã€æ—¥åœ“'
            }
        ]
        
        return upcoming_events
    
    def generate_daily_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ¯æ—¥å…¨çƒå¸‚å ´å ±å‘Š"""
        logger.info("ç”Ÿæˆæ¯æ—¥å…¨çƒå¸‚å ´å ±å‘Š...")
        
        try:
            # ç²å–å„é …åˆ†æ
            sentiment_analysis = self.analyze_global_sentiment()
            indices_data = self.get_global_indices_data()
            correlation_analysis = self.get_correlation_analysis()
            economic_calendar = self.get_economic_calendar()
            
            # ç”Ÿæˆæ‘˜è¦
            summary_points = []
            
            if 'global_sentiment_score' in sentiment_analysis:
                score = sentiment_analysis['global_sentiment_score']
                sentiment = sentiment_analysis['sentiment']
                summary_points.append(f"å…¨çƒå¸‚å ´æƒ…ç·’{sentiment} (è©•åˆ†: {score:.1f})")
                
                # VIX åˆ†æ
                vix_value = sentiment_analysis.get('vix_value', 0)
                if vix_value > 25:
                    summary_points.append(f"VIXæŒ‡æ•¸ {vix_value:.1f} é¡¯ç¤ºå¸‚å ´ææ…Œæƒ…ç·’è¼ƒé«˜")
                elif vix_value < 15:
                    summary_points.append(f"VIXæŒ‡æ•¸ {vix_value:.1f} é¡¯ç¤ºå¸‚å ´æƒ…ç·’ç©©å®š")
                
                # å€åŸŸè¡¨ç¾
                regional_perf = sentiment_analysis.get('regional_performance', {})
                best_region = max(regional_perf.items(), key=lambda x: x[1]['avg_change']) if regional_perf else None
                if best_region:
                    summary_points.append(f"{best_region[0]}å¸‚å ´è¡¨ç¾æœ€ä½³ (+{best_region[1]['avg_change']:.1f}%)")
            
            # å°è‚¡ç›¸é—œå»ºè­°
            taiwan_suggestions = []
            if indices_data.get('SPX', {}).get('change_percent', 0) > 1:
                taiwan_suggestions.append("ç¾è‚¡å¼·å‹¢ï¼Œå°è‚¡æœ‰æœ›è·Ÿéš¨ä¸Šæ¼²")
            if indices_data.get('NIKKEI', {}).get('change_percent', 0) > 1:
                taiwan_suggestions.append("æ—¥è‚¡ä¸Šæ¼²ï¼Œäºæ´²å¸‚å ´æ°£æ°›æ­£é¢")
            
            report = {
                'report_date': datetime.now().strftime('%Y-%m-%d'),
                'summary': 'ã€‚'.join(summary_points),
                'sentiment_analysis': sentiment_analysis,
                'indices_data': indices_data,
                'correlation_analysis': correlation_analysis,
                'economic_calendar': economic_calendar,
                'taiwan_investment_suggestions': taiwan_suggestions,
                'generated_time': datetime.now().isoformat()
            }
            
            return report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¯æ—¥å ±å‘Šå¤±æ•—: {e}")
            return {
                'error': 'ç”Ÿæˆå ±å‘Šå¤±æ•—',
                'report_date': datetime.now().strftime('%Y-%m-%d'),
                'generated_time': datetime.now().isoformat()
            }
    
    def _load_cache(self, key: str) -> Optional[Dict[str, Any]]:
        """è¼‰å…¥å¿«å–"""
        cache_file = os.path.join(self.cache_dir, f"{key}.json")
        
        if not os.path.exists(cache_file):
            return None
        
        try:
            # æª¢æŸ¥å¿«å–æ˜¯å¦éæœŸ
            file_time = os.path.getmtime(cache_file)
            current_time = time.time()
            
            # æ ¹æ“šæ™‚é–“æ±ºå®šå¿«å–æœŸé™
            now = datetime.now()
            if now.weekday() >= 5:  # é€±æœ«
                cache_limit = self.cache_duration['weekend'] * 60
            elif 9 <= now.hour <= 17:  # ç‡Ÿæ¥­æ™‚é–“
                cache_limit = self.cache_duration['market_hours'] * 60
            else:  # ç›¤å¾Œ
                cache_limit = self.cache_duration['after_hours'] * 60
            
            if (current_time - file_time) > cache_limit:
                return None
            
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            logger.error(f"è¼‰å…¥å¿«å–å¤±æ•—: {e}")
            return None
    
    def _save_cache(self, key: str, data: Dict[str, Any]) -> None:
        """ä¿å­˜å¿«å–"""
        cache_file = os.path.join(self.cache_dir, f"{key}.json")
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜å¿«å–å¤±æ•—: {e}")


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    monitor = GlobalEconomicMonitor()
    
    print("ğŸŒ å…¨çƒç¶“æ¿ŸæŒ‡æ¨™ç›£æ§ç³»çµ±æ¸¬è©¦")
    print("=" * 50)
    
    # æ¸¬è©¦ç²å–å…¨çƒæŒ‡æ•¸
    print("ğŸ“Š ç²å–å…¨çƒæŒ‡æ•¸æ•¸æ“š...")
    indices = monitor.get_global_indices_data()
    if indices:
        print(f"æˆåŠŸç²å– {len(indices)} å€‹æŒ‡æ•¸")
        for code, data in list(indices.items())[:5]:
            print(f"  {data['name']}: {data['price']} ({data['change_percent']:+.2f}%)")
    
    # æ¸¬è©¦æƒ…ç·’åˆ†æ
    print(f"\nğŸ“ˆ å…¨çƒå¸‚å ´æƒ…ç·’åˆ†æ...")
    sentiment = monitor.analyze_global_sentiment()
    if 'global_sentiment_score' in sentiment:
        print(f"å…¨çƒæƒ…ç·’è©•åˆ†: {sentiment['global_sentiment_score']:.2f}")
        print(f"æƒ…ç·’ç‹€æ…‹: {sentiment['sentiment']}")
    
    # æ¸¬è©¦æ¯æ—¥å ±å‘Š
    print(f"\nğŸ“‹ ç”Ÿæˆæ¯æ—¥å ±å‘Š...")
    report = monitor.generate_daily_report()
    if 'summary' in report:
        print(f"å ±å‘Šæ‘˜è¦: {report['summary']}")
    
    print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼")
