"""
ml_models.py - æ©Ÿå™¨å­¸ç¿’é æ¸¬æ¨¡å‹
æ•´åˆ XGBoostã€LightGBMã€éš¨æ©Ÿæ£®æ—ç­‰æ¨¡å‹

åŠŸèƒ½ï¼š
1. ç‰¹å¾µå·¥ç¨‹
2. æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬
3. é›†æˆå­¸ç¿’
4. æ¨¡å‹è©•ä¼°èˆ‡é¸æ“‡
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import logging
import json
import os
import pickle
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

# å˜—è©¦å°å…¥ ML åº«ï¼ˆå¦‚æœæ²’æœ‰å®‰è£å‰‡ä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬ï¼‰
try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("sklearn æœªå®‰è£ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬")

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    logger.info("XGBoost æœªå®‰è£")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    logger.info("LightGBM æœªå®‰è£")


# ==================== ç‰¹å¾µå·¥ç¨‹ ====================

class FeatureEngineer:
    """
    ç‰¹å¾µå·¥ç¨‹å™¨
    å¾åŸå§‹æ•¸æ“šæå–é æ¸¬ç‰¹å¾µ
    """

    def __init__(self):
        self.feature_names = []

    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å‰µå»ºæ‰€æœ‰ç‰¹å¾µ

        è¼¸å…¥: DataFrame with columns [date, open, high, low, close, volume]
        è¼¸å‡º: DataFrame with all features
        """
        if len(df) < 30:
            logger.warning("æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•å‰µå»ºå®Œæ•´ç‰¹å¾µ")
            return pd.DataFrame()

        features = pd.DataFrame(index=df.index)

        # 1. åƒ¹æ ¼ç‰¹å¾µ
        features = self._add_price_features(df, features)

        # 2. æŠ€è¡“æŒ‡æ¨™ç‰¹å¾µ
        features = self._add_technical_features(df, features)

        # 3. æˆäº¤é‡ç‰¹å¾µ
        features = self._add_volume_features(df, features)

        # 4. å‹•èƒ½ç‰¹å¾µ
        features = self._add_momentum_features(df, features)

        # 5. æ³¢å‹•ç‡ç‰¹å¾µ
        features = self._add_volatility_features(df, features)

        # 6. é€±æœŸç‰¹å¾µ
        features = self._add_cyclical_features(df, features)

        # ç§»é™¤ç©ºå€¼
        features = features.dropna()

        self.feature_names = features.columns.tolist()

        return features

    def _add_price_features(self, df: pd.DataFrame, features: pd.DataFrame) -> pd.DataFrame:
        """åƒ¹æ ¼ç›¸é—œç‰¹å¾µ"""
        close = df['close']

        # åƒ¹æ ¼è®ŠåŒ–
        features['return_1d'] = close.pct_change(1)
        features['return_5d'] = close.pct_change(5)
        features['return_10d'] = close.pct_change(10)
        features['return_20d'] = close.pct_change(20)

        # åƒ¹æ ¼ä½ç½®
        features['price_to_high_20d'] = close / close.rolling(20).max()
        features['price_to_low_20d'] = close / close.rolling(20).min()
        features['price_range_20d'] = (close.rolling(20).max() - close.rolling(20).min()) / close

        # ç¼ºå£
        features['gap'] = df['open'] / close.shift(1) - 1

        # å¯¦é«”/å½±ç·š
        body = abs(df['close'] - df['open'])
        upper_shadow = df['high'] - df[['open', 'close']].max(axis=1)
        lower_shadow = df[['open', 'close']].min(axis=1) - df['low']
        features['body_ratio'] = body / (df['high'] - df['low'] + 0.001)
        features['upper_shadow_ratio'] = upper_shadow / (df['high'] - df['low'] + 0.001)
        features['lower_shadow_ratio'] = lower_shadow / (df['high'] - df['low'] + 0.001)

        return features

    def _add_technical_features(self, df: pd.DataFrame, features: pd.DataFrame) -> pd.DataFrame:
        """æŠ€è¡“æŒ‡æ¨™ç‰¹å¾µ"""
        close = df['close']
        high = df['high']
        low = df['low']

        # ç§»å‹•å¹³å‡ç·š
        for period in [5, 10, 20, 60]:
            ma = close.rolling(period).mean()
            features[f'ma_{period}_ratio'] = close / ma
            features[f'ma_{period}_slope'] = ma.pct_change(5)

        # MA äº¤å‰
        ma5 = close.rolling(5).mean()
        ma20 = close.rolling(20).mean()
        features['ma_cross'] = (ma5 - ma20) / close

        # RSI
        delta = close.diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / (loss + 0.001)
        features['rsi'] = 100 - (100 / (1 + rs))
        features['rsi_ma'] = features['rsi'].rolling(5).mean()

        # MACD
        exp12 = close.ewm(span=12, adjust=False).mean()
        exp26 = close.ewm(span=26, adjust=False).mean()
        macd = exp12 - exp26
        signal = macd.ewm(span=9, adjust=False).mean()
        features['macd'] = macd / close
        features['macd_signal'] = signal / close
        features['macd_hist'] = (macd - signal) / close

        # å¸ƒæ—å¸¶
        bb_mid = close.rolling(20).mean()
        bb_std = close.rolling(20).std()
        features['bb_upper'] = (bb_mid + 2 * bb_std) / close
        features['bb_lower'] = (bb_mid - 2 * bb_std) / close
        features['bb_width'] = 4 * bb_std / bb_mid
        features['bb_position'] = (close - bb_mid) / (2 * bb_std + 0.001)

        # KD
        low_min = low.rolling(9).min()
        high_max = high.rolling(9).max()
        rsv = (close - low_min) / (high_max - low_min + 0.001) * 100
        features['k'] = rsv.ewm(span=3, adjust=False).mean()
        features['d'] = features['k'].ewm(span=3, adjust=False).mean()
        features['kd_cross'] = features['k'] - features['d']

        # å¨å»‰æŒ‡æ¨™
        features['williams_r'] = (high_max - close) / (high_max - low_min + 0.001) * -100

        # ATR
        tr1 = high - low
        tr2 = abs(high - close.shift(1))
        tr3 = abs(low - close.shift(1))
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        features['atr'] = tr.rolling(14).mean() / close
        features['atr_ratio'] = tr / tr.rolling(14).mean()

        # CCI
        tp = (high + low + close) / 3
        tp_ma = tp.rolling(20).mean()
        tp_std = tp.rolling(20).std()
        features['cci'] = (tp - tp_ma) / (0.015 * tp_std + 0.001)

        return features

    def _add_volume_features(self, df: pd.DataFrame, features: pd.DataFrame) -> pd.DataFrame:
        """æˆäº¤é‡ç‰¹å¾µ"""
        volume = df['volume']
        close = df['close']

        # æˆäº¤é‡è®ŠåŒ–
        features['volume_change'] = volume.pct_change()
        features['volume_ma5_ratio'] = volume / volume.rolling(5).mean()
        features['volume_ma20_ratio'] = volume / volume.rolling(20).mean()

        # é‡åƒ¹é—œä¿‚
        features['volume_price_trend'] = (volume * close.pct_change()).cumsum()
        features['volume_price_corr'] = close.pct_change().rolling(10).corr(volume.pct_change())

        # OBV
        obv_direction = np.sign(close.diff())
        features['obv'] = (obv_direction * volume).cumsum()
        features['obv_ma'] = features['obv'].rolling(10).mean()
        features['obv_slope'] = features['obv'].pct_change(5)

        # æˆäº¤é‡è¶¨å‹¢
        features['volume_trend'] = volume.rolling(5).mean() / volume.rolling(20).mean()

        return features

    def _add_momentum_features(self, df: pd.DataFrame, features: pd.DataFrame) -> pd.DataFrame:
        """å‹•èƒ½ç‰¹å¾µ"""
        close = df['close']

        # ROC (Rate of Change)
        for period in [5, 10, 20]:
            features[f'roc_{period}'] = (close - close.shift(period)) / close.shift(period)

        # å‹•é‡
        features['momentum_5'] = close - close.shift(5)
        features['momentum_10'] = close - close.shift(10)

        # åŠ é€Ÿåº¦
        features['acceleration'] = features['return_1d'] - features['return_1d'].shift(1)

        # è¶¨å‹¢å¼·åº¦
        up_moves = (close.diff() > 0).astype(int).rolling(20).sum()
        features['trend_strength'] = up_moves / 20

        return features

    def _add_volatility_features(self, df: pd.DataFrame, features: pd.DataFrame) -> pd.DataFrame:
        """æ³¢å‹•ç‡ç‰¹å¾µ"""
        close = df['close']

        # æ­·å²æ³¢å‹•ç‡
        features['volatility_5d'] = close.pct_change().rolling(5).std() * np.sqrt(252)
        features['volatility_20d'] = close.pct_change().rolling(20).std() * np.sqrt(252)

        # æ³¢å‹•ç‡è®ŠåŒ–
        features['volatility_ratio'] = features['volatility_5d'] / (features['volatility_20d'] + 0.001)

        # Parkinson æ³¢å‹•ç‡
        high = df['high']
        low = df['low']
        parkinson = np.sqrt((1 / (4 * np.log(2))) * (np.log(high / low) ** 2))
        features['parkinson_vol'] = parkinson.rolling(20).mean()

        return features

    def _add_cyclical_features(self, df: pd.DataFrame, features: pd.DataFrame) -> pd.DataFrame:
        """é€±æœŸç‰¹å¾µ"""
        if 'date' in df.columns:
            dates = pd.to_datetime(df['date'])
        else:
            dates = df.index

        # æ˜ŸæœŸå¹¾
        features['day_of_week'] = dates.dayofweek / 4  # æ­£è¦åŒ–åˆ° 0-1

        # æœˆä»½
        features['month'] = dates.month / 12  # æ­£è¦åŒ–åˆ° 0-1

        # æœˆåˆ/æœˆæœ«æ•ˆæ‡‰
        features['is_month_start'] = (dates.day <= 5).astype(int)
        features['is_month_end'] = (dates.day >= 25).astype(int)

        return features

    def create_target(self, df: pd.DataFrame, forward_days: int = 5,
                      threshold: float = 0.02) -> pd.Series:
        """
        å‰µå»ºé æ¸¬ç›®æ¨™

        Args:
            df: åƒ¹æ ¼æ•¸æ“š
            forward_days: å‘å‰çœ‹çš„å¤©æ•¸
            threshold: æ¼²è·Œåˆ¤æ–·é–¾å€¼

        Returns:
            Series: 1=ä¸Šæ¼², 0=æŒå¹³, -1=ä¸‹è·Œ
        """
        future_return = df['close'].shift(-forward_days) / df['close'] - 1

        target = pd.Series(0, index=df.index)
        target[future_return > threshold] = 1
        target[future_return < -threshold] = -1

        return target


# ==================== ML æ¨¡å‹åŒ…è£å™¨ ====================

class MLModelWrapper:
    """
    æ©Ÿå™¨å­¸ç¿’æ¨¡å‹åŒ…è£å™¨
    çµ±ä¸€æ¥å£ç®¡ç†ä¸åŒçš„ ML æ¨¡å‹
    """

    def __init__(self, model_type: str = 'auto'):
        """
        Args:
            model_type: 'xgboost', 'lightgbm', 'random_forest', 'gradient_boosting', 'auto'
        """
        self.model_type = model_type
        self.model = None
        self.scaler = StandardScaler() if SKLEARN_AVAILABLE else None
        self.feature_names = []
        self.is_trained = False

        self._init_model()

    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self.model_type == 'auto':
            # è‡ªå‹•é¸æ“‡æœ€ä½³å¯ç”¨æ¨¡å‹
            if XGBOOST_AVAILABLE:
                self.model_type = 'xgboost'
            elif LIGHTGBM_AVAILABLE:
                self.model_type = 'lightgbm'
            elif SKLEARN_AVAILABLE:
                self.model_type = 'gradient_boosting'
            else:
                self.model_type = 'simple'

        if self.model_type == 'xgboost' and XGBOOST_AVAILABLE:
            self.model = xgb.XGBClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                use_label_encoder=False,
                eval_metric='mlogloss'
            )
            logger.info("ä½¿ç”¨ XGBoost æ¨¡å‹")

        elif self.model_type == 'lightgbm' and LIGHTGBM_AVAILABLE:
            self.model = lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                verbose=-1
            )
            logger.info("ä½¿ç”¨ LightGBM æ¨¡å‹")

        elif self.model_type == 'random_forest' and SKLEARN_AVAILABLE:
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
            logger.info("ä½¿ç”¨ Random Forest æ¨¡å‹")

        elif self.model_type == 'gradient_boosting' and SKLEARN_AVAILABLE:
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            )
            logger.info("ä½¿ç”¨ Gradient Boosting æ¨¡å‹")

        else:
            logger.warning("ä½¿ç”¨ç°¡åŒ–é æ¸¬æ¨¡å‹")
            self.model = None

    def train(self, X: pd.DataFrame, y: pd.Series,
              test_size: float = 0.2) -> Dict[str, float]:
        """
        è¨“ç·´æ¨¡å‹

        Returns:
            Dict: è¨“ç·´æŒ‡æ¨™
        """
        self.feature_names = X.columns.tolist()

        # ç§»é™¤ç©ºå€¼
        valid_idx = ~(X.isna().any(axis=1) | y.isna())
        X_clean = X[valid_idx]
        y_clean = y[valid_idx]

        if len(X_clean) < 50:
            logger.warning("è¨“ç·´æ•¸æ“šä¸è¶³")
            return {'accuracy': 0, 'error': 'insufficient_data'}

        # æ¨™æº–åŒ–
        if self.scaler:
            X_scaled = self.scaler.fit_transform(X_clean)
        else:
            X_scaled = X_clean.values

        # åˆ†å‰²æ•¸æ“š
        if SKLEARN_AVAILABLE:
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y_clean, test_size=test_size, shuffle=False
            )
        else:
            split_idx = int(len(X_scaled) * (1 - test_size))
            X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]
            y_train, y_test = y_clean.iloc[:split_idx], y_clean.iloc[split_idx:]

        # è¨“ç·´
        if self.model:
            self.model.fit(X_train, y_train)
            y_pred = self.model.predict(X_test)

            metrics = {
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),
                'recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),
                'f1': f1_score(y_test, y_pred, average='weighted', zero_division=0),
                'train_size': len(X_train),
                'test_size': len(X_test)
            }

            self.is_trained = True
            logger.info(f"æ¨¡å‹è¨“ç·´å®Œæˆ - æº–ç¢ºç‡: {metrics['accuracy']:.4f}")

            return metrics
        else:
            return {'accuracy': 0, 'error': 'no_model'}

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é æ¸¬"""
        if not self.is_trained or self.model is None:
            # è¿”å›é è¨­é æ¸¬
            return np.zeros(len(X))

        # ç¢ºä¿ç‰¹å¾µé †åºä¸€è‡´
        X_aligned = X[self.feature_names] if all(f in X.columns for f in self.feature_names) else X

        # æ¨™æº–åŒ–
        if self.scaler:
            X_scaled = self.scaler.transform(X_aligned)
        else:
            X_scaled = X_aligned.values

        return self.model.predict(X_scaled)

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """é æ¸¬æ©Ÿç‡"""
        if not self.is_trained or self.model is None:
            return np.full((len(X), 3), 1/3)

        X_aligned = X[self.feature_names] if all(f in X.columns for f in self.feature_names) else X

        if self.scaler:
            X_scaled = self.scaler.transform(X_aligned)
        else:
            X_scaled = X_aligned.values

        return self.model.predict_proba(X_scaled)

    def get_feature_importance(self) -> Dict[str, float]:
        """ç²å–ç‰¹å¾µé‡è¦æ€§"""
        if not self.is_trained or self.model is None:
            return {}

        if hasattr(self.model, 'feature_importances_'):
            importance = self.model.feature_importances_
            return dict(zip(self.feature_names, importance))

        return {}

    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'model_type': self.model_type,
            'is_trained': self.is_trained
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜: {filepath}")

    def load_model(self, filepath: str):
        """è¼‰å…¥æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_names = model_data['feature_names']
        self.model_type = model_data['model_type']
        self.is_trained = model_data['is_trained']
        logger.info(f"æ¨¡å‹å·²è¼‰å…¥: {filepath}")


# ==================== é›†æˆé æ¸¬å™¨ ====================

class EnsemblePredictor:
    """
    é›†æˆé æ¸¬å™¨
    çµåˆå¤šå€‹æ¨¡å‹çš„é æ¸¬çµæœ
    """

    def __init__(self):
        self.models = {}
        self.feature_engineer = FeatureEngineer()
        self.model_weights = {}

    def add_model(self, name: str, model: MLModelWrapper, weight: float = 1.0):
        """æ·»åŠ æ¨¡å‹"""
        self.models[name] = model
        self.model_weights[name] = weight

    def train_all(self, df: pd.DataFrame, forward_days: int = 5) -> Dict[str, Dict]:
        """è¨“ç·´æ‰€æœ‰æ¨¡å‹"""
        # å‰µå»ºç‰¹å¾µ
        features = self.feature_engineer.create_features(df)
        target = self.feature_engineer.create_target(df, forward_days)

        # å°é½Šæ•¸æ“š
        common_idx = features.index.intersection(target.dropna().index)
        X = features.loc[common_idx]
        y = target.loc[common_idx]

        results = {}
        for name, model in self.models.items():
            logger.info(f"è¨“ç·´æ¨¡å‹: {name}")
            metrics = model.train(X, y)
            results[name] = metrics

            # æ ¹æ“šæº–ç¢ºç‡èª¿æ•´æ¬Šé‡
            if metrics.get('accuracy', 0) > 0:
                self.model_weights[name] = metrics['accuracy']

        return results

    def predict(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        é›†æˆé æ¸¬

        Returns:
            {
                'prediction': int,  # -1, 0, 1
                'probability': Dict,  # å„é¡åˆ¥æ©Ÿç‡
                'confidence': float,  # ä¿¡å¿ƒåº¦
                'individual_predictions': Dict  # å„æ¨¡å‹é æ¸¬
            }
        """
        features = self.feature_engineer.create_features(df)

        if len(features) == 0:
            return {'prediction': 0, 'probability': {}, 'confidence': 0}

        # å–æœ€æ–°ä¸€ç­†
        X = features.tail(1)

        predictions = {}
        probabilities = {}

        for name, model in self.models.items():
            pred = model.predict(X)
            proba = model.predict_proba(X)
            predictions[name] = int(pred[0])
            probabilities[name] = proba[0].tolist()

        # åŠ æ¬ŠæŠ•ç¥¨
        if predictions:
            weighted_votes = {-1: 0, 0: 0, 1: 0}
            total_weight = sum(self.model_weights.values())

            for name, pred in predictions.items():
                weight = self.model_weights.get(name, 1.0)
                weighted_votes[pred] += weight

            final_pred = max(weighted_votes, key=weighted_votes.get)
            confidence = weighted_votes[final_pred] / total_weight if total_weight > 0 else 0

            # è¨ˆç®—å¹³å‡æ©Ÿç‡
            avg_proba = np.mean(list(probabilities.values()), axis=0)

            return {
                'prediction': final_pred,
                'probability': {
                    'down': avg_proba[0] if len(avg_proba) > 0 else 0,
                    'neutral': avg_proba[1] if len(avg_proba) > 1 else 0,
                    'up': avg_proba[2] if len(avg_proba) > 2 else 0
                },
                'confidence': confidence,
                'individual_predictions': predictions
            }

        return {'prediction': 0, 'probability': {}, 'confidence': 0}


# ==================== å¿«é€Ÿé æ¸¬å™¨ï¼ˆä¸éœ€è¦è¨“ç·´ï¼‰====================

class QuickPredictor:
    """
    å¿«é€Ÿé æ¸¬å™¨
    ä½¿ç”¨è¦å‰‡å’Œçµ±è¨ˆæ–¹æ³•ï¼Œä¸éœ€è¦è¨“ç·´
    """

    def __init__(self):
        self.feature_engineer = FeatureEngineer()

    def predict(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        å¿«é€Ÿé æ¸¬

        åŸºæ–¼æŠ€è¡“æŒ‡æ¨™çµ„åˆçµ¦å‡ºé æ¸¬
        """
        if len(df) < 30:
            return {'prediction': 0, 'confidence': 0, 'signals': []}

        features = self.feature_engineer.create_features(df)
        if len(features) == 0:
            return {'prediction': 0, 'confidence': 0, 'signals': []}

        latest = features.iloc[-1]
        signals = []
        score = 0

        # RSI ä¿¡è™Ÿ
        if 'rsi' in latest:
            rsi = latest['rsi']
            if rsi < 30:
                signals.append(('RSI è¶…è³£', 2))
                score += 2
            elif rsi > 70:
                signals.append(('RSI è¶…è²·', -2))
                score -= 2

        # MACD ä¿¡è™Ÿ
        if 'macd_hist' in latest:
            macd_hist = latest['macd_hist']
            if macd_hist > 0:
                signals.append(('MACD æ­£å€¼', 1))
                score += 1
            else:
                signals.append(('MACD è² å€¼', -1))
                score -= 1

        # å‡ç·šä¿¡è™Ÿ
        if 'ma_cross' in latest:
            ma_cross = latest['ma_cross']
            if ma_cross > 0.02:
                signals.append(('çŸ­æœŸå‡ç·šä¸Šç©¿', 1.5))
                score += 1.5
            elif ma_cross < -0.02:
                signals.append(('çŸ­æœŸå‡ç·šä¸‹ç©¿', -1.5))
                score -= 1.5

        # KD ä¿¡è™Ÿ
        if 'kd_cross' in latest:
            kd = latest['kd_cross']
            if kd > 10:
                signals.append(('KD é‡‘å‰', 1))
                score += 1
            elif kd < -10:
                signals.append(('KD æ­»å‰', -1))
                score -= 1

        # å¸ƒæ—å¸¶ä¿¡è™Ÿ
        if 'bb_position' in latest:
            bb = latest['bb_position']
            if bb < -1:
                signals.append(('è·Œç ´å¸ƒæ—ä¸‹è»Œ', 1.5))
                score += 1.5
            elif bb > 1:
                signals.append(('çªç ´å¸ƒæ—ä¸Šè»Œ', -0.5))
                score -= 0.5

        # æˆäº¤é‡ä¿¡è™Ÿ
        if 'volume_ma5_ratio' in latest:
            vol_ratio = latest['volume_ma5_ratio']
            if vol_ratio > 2:
                signals.append(('æˆäº¤é‡æ”¾å¤§', 0.5))
                score += 0.5

        # å‹•èƒ½ä¿¡è™Ÿ
        if 'momentum_5' in latest and 'momentum_10' in latest:
            if latest['momentum_5'] > 0 and latest['momentum_10'] > 0:
                signals.append(('å‹•èƒ½å‘ä¸Š', 1))
                score += 1
            elif latest['momentum_5'] < 0 and latest['momentum_10'] < 0:
                signals.append(('å‹•èƒ½å‘ä¸‹', -1))
                score -= 1

        # è¨ˆç®—é æ¸¬
        if score >= 3:
            prediction = 1
        elif score <= -3:
            prediction = -1
        else:
            prediction = 0

        confidence = min(abs(score) / 8, 1.0)

        return {
            'prediction': prediction,
            'score': score,
            'confidence': confidence,
            'signals': [(s[0], s[1]) for s in signals],
            'features': {k: float(v) for k, v in latest.to_dict().items()
                        if not pd.isna(v)}
        }


# ==================== æ¸¬è©¦ ====================

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)

    # å‰µå»ºæ¨¡æ“¬æ•¸æ“š
    np.random.seed(42)
    dates = pd.date_range(start='2024-01-01', periods=200, freq='D')
    prices = 100 * np.cumprod(1 + np.random.normal(0.001, 0.02, 200))

    df = pd.DataFrame({
        'date': dates,
        'open': prices * (1 + np.random.uniform(-0.01, 0.01, 200)),
        'high': prices * (1 + np.abs(np.random.normal(0, 0.015, 200))),
        'low': prices * (1 - np.abs(np.random.normal(0, 0.015, 200))),
        'close': prices,
        'volume': np.random.randint(1000000, 10000000, 200)
    })

    print("=" * 60)
    print("ğŸ“Š ML æ¨¡å‹æ¸¬è©¦")
    print("=" * 60)

    # æ¸¬è©¦ç‰¹å¾µå·¥ç¨‹
    print("\n1. ç‰¹å¾µå·¥ç¨‹æ¸¬è©¦")
    fe = FeatureEngineer()
    features = fe.create_features(df)
    print(f"å‰µå»ºäº† {len(features.columns)} å€‹ç‰¹å¾µ")
    print(f"ç‰¹å¾µåˆ—è¡¨: {features.columns.tolist()[:10]}...")

    # æ¸¬è©¦å¿«é€Ÿé æ¸¬å™¨
    print("\n2. å¿«é€Ÿé æ¸¬å™¨æ¸¬è©¦")
    quick = QuickPredictor()
    result = quick.predict(df)
    print(f"é æ¸¬: {result['prediction']}")
    print(f"ä¿¡å¿ƒåº¦: {result['confidence']:.2f}")
    print(f"ä¿¡è™Ÿ: {result['signals']}")

    # æ¸¬è©¦ ML æ¨¡å‹
    if SKLEARN_AVAILABLE:
        print("\n3. ML æ¨¡å‹æ¸¬è©¦")
        model = MLModelWrapper(model_type='auto')
        target = fe.create_target(df, forward_days=5)

        common_idx = features.index.intersection(target.dropna().index)
        X = features.loc[common_idx]
        y = target.loc[common_idx]

        metrics = model.train(X, y)
        print(f"è¨“ç·´çµæœ: {metrics}")

        # ç‰¹å¾µé‡è¦æ€§
        importance = model.get_feature_importance()
        if importance:
            top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:5]
            print(f"Top 5 é‡è¦ç‰¹å¾µ: {top_features}")
